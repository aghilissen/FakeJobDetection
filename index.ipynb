{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import where\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fake_job_postings.csv\",index_col='job_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we've identified the False Negative to be the errors with the highest cost and because we'd rather use a ROC_AUC score (as well as the ROC curve which has the benefit to help us fine tune the most efficient model), we've decided to swap the values in the 'fraudulent' column. This means the model will now predict if a job post is legitimate and it will allow us to minimise the False positive (when an offer is flagged as legit by the model but actually is of fraudulent nature):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fraudulent'].replace([0,1], [1,0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addressing the missing information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to address the missing information in the different columns by replacing the lack of information by `NA`. Instead of using the 'Other' or 'Unspecified' values available in certain columns, this will allow us to quickly spot when the information was not provided:\n",
    "01. title:               No missing data\n",
    "02. location:            NA\n",
    "03. department:          NA\n",
    "04. salary_range:        NA\n",
    "05. company_profile:     NA\n",
    "06. description:         NA\n",
    "07. requirements:        NA\n",
    "08. benefits:            NA\n",
    "09. telecommuting:       No missing data\n",
    "10. has_company_logo:    No missing data\n",
    "11. has_questions:       No missing data\n",
    "12. employment_type:     NA\n",
    "13. required_experience: NA\n",
    "14. required_education:  NA\n",
    "15. industry:            NA\n",
    "16. function:            NA\n",
    "17. fraudulent:          No missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    df[column].fillna('NA', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Some job offers have contact details or external url. Could this be linked to fraudulent activity? Could this improve our model's performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# turn into a function: use dict (columnName:textToSearch, list(dictinput.keys())[0]:list(dictinput.values())[0])\n",
    "df = df.assign(hasEMAIL=0, hasPHONE=0, hasURL=0)\n",
    "for column in df.columns[4:8]:\n",
    "    for i in range(1,len(df[column])):\n",
    "        if df[column][i].find('#URL_')!=-1:\n",
    "            df['hasURL'][i] = 1\n",
    "        elif df[column][i].find('#PHONE_')!=-1:\n",
    "            df['hasPHONE'][i] = 1\n",
    "        elif df[column][i].find('#EMAIL_')!=-1:\n",
    "            df['hasEMAIL'][i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if any clear pattern can already be identified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df[column].value_counts() for column in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df.drop(columns=['fraudulent']).corr(), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No multicollinearity issues with those binary variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns[4:8]:\n",
    "    for i in range(1,len(df[column])):\n",
    "        df[column][i] = re.sub(r'([A-Z][a-z])', r' \\1', df[column][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['fraudulent']\n",
    "features = df.drop(columns=['fraudulent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = train_test_split(features,\n",
    "                                                    labels,\n",
    "                                                    random_state=666)\n",
    "\n",
    "# Initialising K-Folds\n",
    "kfold = KFold(n_splits=5,\n",
    "              random_state=666,\n",
    "              shuffle=True\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_comp = TfidfVectorizer(input='content', strip_accents='unicode', token_pattern=r'\\w+', analyzer='word', stop_words='english')\n",
    "vect_desc = TfidfVectorizer(input='content', strip_accents='unicode', token_pattern=r'\\w+', analyzer='word', stop_words='english')\n",
    "vect_req = TfidfVectorizer(input='content', strip_accents='unicode', token_pattern=r'\\w+', analyzer='word', stop_words='english')\n",
    "vect_ben = TfidfVectorizer(input='content', strip_accents='unicode', token_pattern=r'\\w+', analyzer='word', stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_vectorized_c = vect_comp.fit_transform([document for document in features_train['company_profile']])\n",
    "features_test_vectorized_c = vect_comp.transform([document for document in features_test['company_profile']])\n",
    "\n",
    "features_train_vectorized_cd = vect_desc.fit_transform([document for document in features_train['description']])\n",
    "features_test_vectorized_cd = vect_desc.transform([document for document in features_test['description']])\n",
    "\n",
    "features_train_vectorized_cdr = vect_req.fit_transform([document for document in features_train['requirements']])\n",
    "features_test_vectorized_cdr = vect_req.transform([document for document in features_test['requirements']])\n",
    "\n",
    "features_train_vectorized_cdrb = vect_ben.fit_transform([document for document in features_train['benefits']])\n",
    "features_test_vectorized_cdrb = vect_ben.transform([document for document in features_test['benefits']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('company_profile in train set:', len([document for document in features_train['company_profile']]))\n",
    "print('shape of densified train [[tfidf]]:', features_train_vectorized_c.todense().shape)\n",
    "print('-'*20)\n",
    "print('company_profile in test set:', len([document for document in features_test['company_profile']]))\n",
    "print('shape of densified test [[tfidf]]:', features_test_vectorized_c.todense().shape)\n",
    "print('-'*40)\n",
    "print('description in train set:', len([document for document in features_train['description']]))\n",
    "print('shape of densified train [[tfidf]]:', features_train_vectorized_cd.todense().shape)\n",
    "print('-'*20)\n",
    "print('description in test set:', len([document for document in features_test['description']]))\n",
    "print('shape of densified test [[tfidf]]:', features_test_vectorized_cd.todense().shape)\n",
    "print('-'*40)\n",
    "print('requirements in train set:', len([document for document in features_train['requirements']]))\n",
    "print('shape of densified train [[tfidf]]:', features_train_vectorized_cdr.todense().shape)\n",
    "print('-'*20)\n",
    "print('requirements in test set:', len([document for document in features_test['requirements']]))\n",
    "print('shape of densified test [[tfidf]]:', features_test_vectorized_cdr.todense().shape)\n",
    "print('-'*40)\n",
    "print('benefits in train set:', len([document for document in features_train['benefits']]))\n",
    "print('shape of densified train [[tfidf]]:', features_train_vectorized_cdrb.todense().shape)\n",
    "print('-'*20)\n",
    "print('benefits in test set:', len([document for document in features_test['benefits']]))\n",
    "print('shape of densified test [[tfidf]]:', features_test_vectorized_cdrb.todense().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addressing the class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As class imbalance will only matter during the training step, SMOTE will only be applied to to train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE doesn't work on text, it needs to be changed to TF-IDF\n",
    "sm = SMOTE(random_state=666, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sm.fit_resample(features_train_vectorized_cdrb, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X', X.shape)\n",
    "print('y', y.shape)\n",
    "print('labels_train', labels_train.value_counts())\n",
    "print('y.value_counts', y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(X.todense(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train_pred = gnb.predict_proba(X.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Accuracy (on train set):', (labels_train_pred==labels_train).sum()/labels_train.count())\n",
    "\n",
    "# Before SMOTE\n",
    "# Accuracy (on train set): 0.5287859202545491\n",
    "# Accuracy (on validation set): 0.5144646585147629"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y, [labels_train_pred[x][1] for x in range(len(labels_train_pred))])\n",
    "print('AUC train dataset: {}'.format(auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Massive overfit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(X.todense(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train_pred = mnb.predict(X.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Accuracy (on train set):', (labels_train_pred==labels_train).sum()/labels_train.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Massive overfit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X.todense(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train_pred = rfc.predict(X.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy (on train set):', (labels_train_pred==labels_train).sum()/labels_train.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Massive overfit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finialising the selected model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = rfc.predict(X_test)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_hat_test)\n",
    "\n",
    "# Create the basic matrix\n",
    "plt.imshow(cnf_matrix,  cmap=plt.cm.Blues) \n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "# Add appropriate axis scales\n",
    "class_names = set(y) # Get class labels to add to matrix\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names, rotation=45)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# Add labels to each cell\n",
    "thresh = cnf_matrix.max() / 2. # Used for text coloring below\n",
    "# Here we iterate through the confusion matrix and append labels to our visualization \n",
    "for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
    "        plt.text(j, i, cnf_matrix[i, j],\n",
    "                 horizontalalignment='center',\n",
    "                 color='white' if cnf_matrix[i, j] > thresh else 'black')\n",
    "\n",
    "# Add a legend\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = {'TP': 0, 'TN': 0, 'FP': 0, 'FN': 0}\n",
    "def conf_matrix(self,y_test, y_hat_test):\n",
    "        for ind, label in enumerate(y_test):\n",
    "            pred = y_hat_test[ind]\n",
    "            if label == 1: \n",
    "                if label == pred:\n",
    "                    cm['TP'] += 1\n",
    "                else:\n",
    "                    cm['FN'] += 1\n",
    "            else:\n",
    "                if label == pred:\n",
    "                    cm['TN'] += 1\n",
    "                else:\n",
    "                    cm['FP'] += 1\n",
    "            self.cm_values = cm\n",
    "        return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix(rfc,y_test,y_hat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildROC(target_train, train_preds, target_test,test_preds):\n",
    "    fpr, tpr, threshold = metrics.roc_curve(target_test, test_preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    fpr1, tpr1, threshold = metrics.roc_curve(target_train, train_preds)\n",
    "    roc_auc1 = metrics.auc(fpr1, tpr1)\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr1, tpr1, 'b', label = 'Train AUC = %0.2f' % roc_auc1)\n",
    "    plt.plot(fpr, tpr, 'b', label = 'Validation AUC = %0.2f' % roc_auc, color = 'g')\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.gcf().savefig('roc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_preds = .predict(X_train)\n",
    "test_preds = .predict(X_test)\n",
    "\n",
    "training_roc_auc = roc_auc_score(y_train, training_preds)\n",
    "test_roc_auc = roc_auc_score(y_test, test_preds)\n",
    "\n",
    "print('Training ROC_AUC: {:.4}%'.format(training_roc_auc * 100))\n",
    "print('Validation ROC_AUC: {:.4}%'.format(test_roc_auc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ROC curve\n",
    "buildROC(y_train, training_preds, y_validate, validate_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annot(fpr,tpr,thr):\n",
    "    k=0\n",
    "    for i,j in zip(fpr,tpr):\n",
    "        if k %75 == 0:\n",
    "            plt.annotate(round(thr[k],2),xy=(i,j), textcoords='data')\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in [[y_train, y_train_score[:,1]],[y_validate, y_validate_score[:,1]],[y_test, y_test_score[:,1]]]:\n",
    "    fpr, tpr, threshold = roc_curve(data[0], data[1])\n",
    "    plt.plot(fpr, tpr)\n",
    "annot(fpr, tpr, threshold)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.ylabel('TPR (power)')\n",
    "plt.xlabel('FPR (alpha)')\n",
    "plt.legend(['train','validation','test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_chosen = 0\n",
    "difference = 0\n",
    "for i in range(len(thresholds)):\n",
    "    temp = tpr[i]-fpr[i]\n",
    "    if temp>difference:\n",
    "        difference=temp\n",
    "        threshold_chosen=thresholds[i]\n",
    "threshold_chosen = round(threshold_chosen,2)\n",
    "print('Best Threshold: ',threshold_chosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The cost function terms are wrong, double check those!\n",
    "\n",
    "# Focusing on a stat that improves your game\n",
    "cTP = 100\n",
    "# Not focusing on a stat that doesn't improve your game\n",
    "cTN = 1\n",
    "# Focusing on a stat that doesn't improve your game\n",
    "cFP = 100\n",
    "# Not focusing on a stat that improves your game\n",
    "cFN = 20\n",
    "\n",
    "prevalence = (cm['TP']+cm['FN']) / (cm['TP']+cm['FP']+cm['TN']+cm['FN'])\n",
    "\n",
    "# Metz coefficient\n",
    "_m = ((1-prevalence)/prevalence) * ((cFP-cTN) / (cFN-cTP))\n",
    "\n",
    "# Using the Zweig & Campbell equation:\n",
    "function_m = cm['TP'] -_m*cm['FP']\n",
    "function_m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
