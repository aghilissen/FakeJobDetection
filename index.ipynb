{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from numpy import where\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fake_job_postings.csv\",index_col='job_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we've identified the False Negative to be the errors with the highest cost and because we'd rather use a ROC_AUC score (as well as the ROC curve which has the benefit to help us fine tune the most efficient model), we've decided to swap the values in the 'fraudulent' column. This means the model will now predict if a job post is legitimate and it will allow us to minimise the False positive (when an offer is flagged as legit by the model but actually is of fraudulent nature):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fraudulent'].replace([0,1], [1,0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addressing the missing information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to address the missing information in the different columns by replacing the lack of information by `NA`. Instead of using the 'Other' or 'Unspecified' values available in certain columns, this will allow us to quickly spot when the information was not provided:\n",
    "01. title:               No missing data\n",
    "02. location:            NA\n",
    "03. department:          NA\n",
    "04. salary_range:        NA\n",
    "05. company_profile:     NA\n",
    "06. description:         NA\n",
    "07. requirements:        NA\n",
    "08. benefits:            NA\n",
    "09. telecommuting:       No missing data\n",
    "10. has_company_logo:    No missing data\n",
    "11. has_questions:       No missing data\n",
    "12. employment_type:     NA\n",
    "13. required_experience: NA\n",
    "14. required_education:  NA\n",
    "15. industry:            NA\n",
    "16. function:            NA\n",
    "17. fraudulent:          No missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    df[column].fillna('NA', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Some job offers have contact details or external url. Could this be linked to fraudulent activity? Could this improve our model's performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# turn into a function: use dict (columnName:textToSearch, list(dictinput.keys())[0]:list(dictinput.values())[0])\n",
    "df = df.assign(hasEMAIL=0, hasPHONE=0, hasURL=0)\n",
    "for column in df.columns[4:8]:\n",
    "    for i in range(1,len(df[column])):\n",
    "        if df[column][i].find('#URL_')!=-1:\n",
    "            df['hasURL'][i] = 1\n",
    "        elif df[column][i].find('#PHONE_')!=-1:\n",
    "            df['hasPHONE'][i] = 1\n",
    "        elif df[column][i].find('#EMAIL_')!=-1:\n",
    "            df['hasEMAIL'][i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if any clear pattern can already be identified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df[column].value_counts() for column in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df.drop(columns=['fraudulent']).corr(), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No multicollinearity issues with those binary variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns[4:8]:\n",
    "    for i in range(1,len(df[column])):\n",
    "        df[column][i] = re.sub(r'([A-Z][a-z])', r' \\1', df[column][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['fraudulent']\n",
    "features = df.drop(columns=['fraudulent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = train_test_split(features,\n",
    "                                                    labels,\n",
    "                                                    random_state=666)\n",
    "\n",
    "# Initialising K-Folds\n",
    "kfold = KFold(n_splits=5,\n",
    "              random_state=666,\n",
    "              shuffle=True\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_comp = TfidfVectorizer(input='content', strip_accents='unicode', token_pattern=r'\\w+', analyzer='word', stop_words='english')\n",
    "vect_desc = TfidfVectorizer(input='content', strip_accents='unicode', token_pattern=r'\\w+', analyzer='word', stop_words='english')\n",
    "vect_req = TfidfVectorizer(input='content', strip_accents='unicode', token_pattern=r'\\w+', analyzer='word', stop_words='english')\n",
    "vect_ben = TfidfVectorizer(input='content', strip_accents='unicode', token_pattern=r'\\w+', analyzer='word', stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_vectorized_c = vect_comp.fit_transform([document for document in features_train['company_profile']])\n",
    "features_test_vectorized_c = vect_comp.transform([document for document in features_test['company_profile']])\n",
    "\n",
    "features_train_vectorized_cd = vect_desc.fit_transform([document for document in features_train['description']])\n",
    "features_test_vectorized_cd = vect_desc.transform([document for document in features_test['description']])\n",
    "\n",
    "features_train_vectorized_cdr = vect_req.fit_transform([document for document in features_train['requirements']])\n",
    "features_test_vectorized_cdr = vect_req.transform([document for document in features_test['requirements']])\n",
    "\n",
    "features_train_vectorized_cdrb = vect_ben.fit_transform([document for document in features_train['benefits']])\n",
    "features_test_vectorized_cdrb = vect_ben.transform([document for document in features_test['benefits']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('company_profile in train set:', len([document for document in features_train['company_profile']]))\n",
    "print('shape of densified train [[tfidf]]:', features_train_vectorized_c.todense().shape)\n",
    "print('-'*20)\n",
    "print('company_profile in test set:', len([document for document in features_test['company_profile']]))\n",
    "print('shape of densified test [[tfidf]]:', features_test_vectorized_c.todense().shape)\n",
    "print('-'*40)\n",
    "print('description in train set:', len([document for document in features_train['description']]))\n",
    "print('shape of densified train [[tfidf]]:', features_train_vectorized_cd.todense().shape)\n",
    "print('-'*20)\n",
    "print('description in test set:', len([document for document in features_test['description']]))\n",
    "print('shape of densified test [[tfidf]]:', features_test_vectorized_cd.todense().shape)\n",
    "print('-'*40)\n",
    "print('requirements in train set:', len([document for document in features_train['requirements']]))\n",
    "print('shape of densified train [[tfidf]]:', features_train_vectorized_cdr.todense().shape)\n",
    "print('-'*20)\n",
    "print('requirements in test set:', len([document for document in features_test['requirements']]))\n",
    "print('shape of densified test [[tfidf]]:', features_test_vectorized_cdr.todense().shape)\n",
    "print('-'*40)\n",
    "print('benefits in train set:', len([document for document in features_train['benefits']]))\n",
    "print('shape of densified train [[tfidf]]:', features_train_vectorized_cdrb.todense().shape)\n",
    "print('-'*20)\n",
    "print('benefits in test set:', len([document for document in features_test['benefits']]))\n",
    "print('shape of densified test [[tfidf]]:', features_test_vectorized_cdrb.todense().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addressing the class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As class imbalance will only matter during the training step, SMOTE will only be applied to to train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE doesn't work on text, it needs to be changed to TF-IDF\n",
    "sm = SMOTE(random_state=666, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sm.fit_resample(features_train_vectorized_cdrb, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape # (19184, 9915)\n",
    "y.shape # (19184,)\n",
    "labels_train.value_counts() # 9592/465\n",
    "y.value_counts() # 9592/9592"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(X.todense(), labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train_pred = gnb.predict(X.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Accuracy (on train set):', (labels_train_pred==labels_train).sum()/labels_train.count())\n",
    "\n",
    "# Before SMOTE\n",
    "# Accuracy (on train set): 0.5287859202545491\n",
    "# Accuracy (on validation set): 0.5144646585147629"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Massive overfit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(X.todense(), labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train_pred = mnb.predict(X.todense())\n",
    "labels_validate_pred = mnb.predict(features_validate_vectorized_cdrb.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Accuracy (on train set):', (labels_train_pred==labels_train).sum()/labels_train.count())\n",
    "print('Accuracy (on validation set):', (labels_validate_pred==labels_validate).sum()/labels_validate.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Massive overfit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X.todense(), labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train_pred = rfc.predict(X.todense())\n",
    "labels_validate_pred = rfc.predict(features_validate_vectorized_cdrb.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy (on train set):', (labels_train_pred==labels_train).sum()/labels_train.count())\n",
    "print('Accuracy (on validation set):', (labels_validate_pred==labels_validate).sum()/labels_validate.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Massive overfit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA following TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[text for text in dffraud[dffraud['requirements'].str.find('URL')!=-1]['requirements']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
